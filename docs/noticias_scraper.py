import requests
from bs4 import BeautifulSoup
import trafilatura
import pandas as pd
import time
import re

# -------------------------------
# CONFIGURACI√ìN
# -------------------------------
# Lista de empresas a buscar
queries = [
    "Sumup Empresas",
]

# C√≥digo de pa√≠s/mercado para la b√∫squeda de Bing News.
# Algunos ejemplos:
#   "es-ES" -> Espa√±ol (Espa√±a)
#   "es-MX" -> Espa√±ol (M√©xico)
#   "en-US" -> Ingl√©s (Estados Unidos)
#   "en-GB" -> Ingl√©s (Reino Unido)
#   "fr-FR" -> Franc√©s (Francia)
pais_filtro = "es-ES"

# N√∫mero m√°ximo de p√°ginas de resultados de Bing News por empresa
max_paginas = 20

# Encabezados para la solicitud HTTP para evitar ser bloqueado
headers = {"User-Agent": "Mozilla/5.0"}

# Lista para almacenar todos los resultados de las noticias
todos_resultados = []

# -------------------------------
# PROCESAMIENTO POR EMPRESA
# -------------------------------
for query in queries:
    print(f"\nüîç Procesando empresa: {query}")
    # Conjunto para evitar duplicados por URL o t√≠tulo
    vistos = set()

    for pagina in range(max_paginas):
        offset = pagina * 10
        # Codifica la consulta para la URL
        encoded_query = query.replace(' ', '+')
        # URL de b√∫squeda en Bing News, ahora incluye el filtro de pa√≠s (setmkt)
        url_busqueda = f"https://www.bing.com/news/search?q={encoded_query}&first={offset}&form=QBNH&setmkt={pais_filtro}"

        print(f"  üîé P√°gina {pagina + 1}: {url_busqueda}")

        try:
            # Realiza la solicitud HTTP
            res = requests.get(url_busqueda, headers=headers, timeout=10)
            # Analiza el HTML
            soup = BeautifulSoup(res.text, "html.parser")
        except requests.exceptions.RequestException as e:
            print(f"  ‚ùå Error al obtener la p√°gina: {e}")
            continue

        # Selecciona los enlaces a las noticias, que suelen tener la clase 'title'
        articulos = soup.select("a.title")

        if not articulos:
            print("  ‚ö†Ô∏è No hay m√°s resultados en esta p√°gina o no se encontraron noticias.")
            break

        for art in articulos:
            titulo = art.get_text(strip=True)
            url = art.get("href")

            # Omite si la URL o el t√≠tulo ya han sido procesados para evitar duplicados
            if url in vistos or titulo in vistos:
                continue
            vistos.add(url)
            vistos.add(titulo)

            print(f"    ‚û°Ô∏è {titulo}")

            # -----------------------------------
            # EXTRACCI√ìN DE CONTENIDO Y FECHA
            # -----------------------------------
            try:
                # Descarga el HTML de la noticia
                raw_html = trafilatura.fetch_url(url)
                if raw_html:
                    # Extrae el contenido principal del art√≠culo
                    contenido = trafilatura.extract(raw_html)
                    
                    # Extrae los metadatos para obtener la fecha de publicaci√≥n
                    metadata = trafilatura.extract_metadata(raw_html)
                    fecha = metadata.date if metadata and metadata.date else None
                else:
                    contenido = None
                    fecha = None

            except Exception as e:
                print(f"    ‚ùå Error extrayendo contenido y fecha de {url}: {e}")
                contenido = None
                fecha = None

            # Agrega los resultados a la lista
            todos_resultados.append({
                "empresa": query,
                "titulo": titulo,
                "url": url,
                "contenido": contenido if contenido else "No extra√≠do",
                "fecha_publicacion": fecha if fecha else "No extra√≠da",
                "pais": pais_filtro  # Agrega el pa√≠s al resultado
            })

            # Pausa de 1 segundo para evitar ser bloqueado por el servidor
            time.sleep(1)

# -------------------------------
# GUARDAR RESULTADOS
# -------------------------------
df = pd.DataFrame(todos_resultados)
# Guarda el DataFrame en un archivo CSV
df.to_csv("noticias_empresas_unificado.csv", index=False, encoding="utf-8-sig")
print(f"\n‚úÖ Guardado completo: {len(df)} art√≠culos en 'noticias_empresas_unificado.csv'")
